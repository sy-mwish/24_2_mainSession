{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "- 일단 중복 고려 안함. 이동 시 발생하는 reward 0.\n",
        "- 오직 게임이 끝나고 win, lose, draw에 따라서만 reward 발생\n",
        "- 몬테카를로 구현~~"
      ],
      "metadata": {
        "id": "NEQfh9B2ofMw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "KWFBBg-ZjXOf"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Tuple\n",
        "from collections import deque\n",
        "import copy\n",
        "from scipy.special import softmax\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 틱택토 환경"
      ],
      "metadata": {
        "id": "nZPH06v7jkS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Environment:\n",
        "    def __init__(self):\n",
        "        self.n = 3\n",
        "        self.num_actions = self.n**2\n",
        "        self.present_state = np.zeros((self.n, self.n))\n",
        "        self.action_space = np.arange(self.num_actions)\n",
        "        self.available_actions = np.ones(self.num_actions)\n",
        "        self.reward_dict = {'win':1, 'lose':-1, 'draw': -0.1, 'good_action':0, 'overlapped':0}\n",
        "        self.done = False\n",
        "\n",
        "\n",
        "    def step(self, action_idx:int, max_player:bool):\n",
        "        '''\n",
        "        에이전트가 선택한 action에 따라 주어지는 next_state, reward, done\n",
        "        '''\n",
        "        x, y = np.divmod(action_idx, self.n)\n",
        "\n",
        "        self.present_state[x,y] = max_player*2 -1\n",
        "        next_state = self.present_state\n",
        "        done, is_win = self.is_done(next_state)\n",
        "        reward = self.reward_dict['good_action']\n",
        "        self.available_actions = self.check_available_action(self.present_state)\n",
        "\n",
        "        if done:\n",
        "            if is_win == \"win\":\n",
        "                reward = self.reward_dict['win']\n",
        "            elif is_win == \"lose\":\n",
        "                reward = self.reward_dict['lose']\n",
        "            else:\n",
        "                reward = self.reward_dict['draw']\n",
        "\n",
        "        self.done = done\n",
        "\n",
        "        return next_state, reward, done, is_win\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        '''\n",
        "        게임판 초기화\n",
        "        '''\n",
        "        self.present_state = np.zeros((self.n, self.n))\n",
        "        self.available_actions = np.ones(self.num_actions)\n",
        "        self.done = False\n",
        "\n",
        "\n",
        "    def render(self):\n",
        "        '''\n",
        "        print the current state\n",
        "        '''\n",
        "        render_state = np.array([['.','.','.'],\n",
        "                                ['.','.','.'],\n",
        "                                ['.','.','.']])\n",
        "        render_str = \"\"\n",
        "        for i in range(self.num_actions):\n",
        "            x, y = np.divmod(i, 3)\n",
        "            if self.present_state[x,y] == 1:\n",
        "                render_state[x,y] = 'X'\n",
        "            elif self.present_state[x,y] == -1:\n",
        "                render_state[x,y] = 'O'\n",
        "\n",
        "            render_str += f\" {render_state[x,y]}\"\n",
        "            if (i+1) % 3 == 0:\n",
        "                render_str += \"\\n\" + \"-\"*11 + \"\\n\"\n",
        "            else:\n",
        "                render_str += \" |\"\n",
        "\n",
        "        print(render_str)\n",
        "\n",
        "\n",
        "    def check_available_action(self, state):\n",
        "        '''\n",
        "        현재 state에서 가능한 actions array 반환\n",
        "        '''\n",
        "        impossible_actions = np.argwhere(state.reshape(-1) != 0)\n",
        "        available_actions = np.ones(self.num_actions)\n",
        "        available_actions[impossible_actions] = 0\n",
        "\n",
        "        return available_actions\n",
        "\n",
        "\n",
        "    def is_done(self, state):\n",
        "        '''\n",
        "        틱택토 게임 종료 조건 및 승리 여부 확인하는 함수\n",
        "        '''\n",
        "        is_done, is_win = False, \"null\"\n",
        "\n",
        "        # 무승부 여부 확인\n",
        "        if (state==0).sum()==0:\n",
        "            is_done, is_win = True, \"draw\"\n",
        "\n",
        "        else:\n",
        "            axis_sum = np.concatenate((state.sum(axis=0), state.sum(axis=1)))\n",
        "            diag_sum = np.array([state.trace(), np.fliplr(state).trace()])\n",
        "\n",
        "            sum_array = np.concatenate((axis_sum, diag_sum))\n",
        "            max_sum = np.max(sum_array)\n",
        "            min_sum = np.min(sum_array)\n",
        "\n",
        "            if max_sum == 3:\n",
        "                is_done, is_win = True, \"win\"\n",
        "            elif min_sum == -3:\n",
        "                is_done, is_win = True, \"lose\"\n",
        "            else:\n",
        "                is_done, is_win = False, \"null\"\n",
        "\n",
        "        return is_done, is_win"
      ],
      "metadata": {
        "id": "-Xb7nqY8jisk"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 몬테카를로 알고리즘\n",
        "- 랜덤 탐색 -> 가치함수 업데이트"
      ],
      "metadata": {
        "id": "0HDRLbbFdd7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self, env, max_player:bool):\n",
        "        self.env = env\n",
        "        self.max_player = max_player\n",
        "\n",
        "        self.n = self.env.n\n",
        "        self.num_actions = self.env.num_actions\n",
        "        self.actions = self.env.action_space\n",
        "\n",
        "        self.value_table = np.zeros(self.num_actions)\n",
        "        self.returns = {i: [] for i in range(self.num_actions)}\n",
        "\n",
        "        self.stepsize = STEPSIZE\n",
        "        self.gamma = GAMMA\n",
        "        self.epsilon = EPSILON\n",
        "        self.epsilon_decay = EPSILON_DECAY\n",
        "        self.epsilon_min = EPSILON_MIN\n",
        "\n",
        "\n",
        "    def update_value_table(self, history):\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "        G = 0\n",
        "        for t in reversed(range(len(history))):\n",
        "            action_idx, reward = history[t]\n",
        "\n",
        "            G = self.gamma * G + reward\n",
        "            self.returns[action_idx].append(G)\n",
        "            self.value_table[action_idx] = np.mean(self.returns[action_idx])\n",
        "\n",
        "\n",
        "    def get_action(self, state, available_actions):\n",
        "        available_action = np.where(available_actions != 0)[0]\n",
        "\n",
        "        if (np.random.rand() <= self.epsilon) or (not self.max_player):\n",
        "            act = random.choice(available_action)\n",
        "\n",
        "        else:\n",
        "            available_value = self.value_table * available_actions\n",
        "            act = np.argmax(available_value)\n",
        "\n",
        "        return act"
      ],
      "metadata": {
        "id": "sxvrWiCYdgJW"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## main"
      ],
      "metadata": {
        "id": "-iDjv1fhpsYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "STEPSIZE = 0.1\n",
        "GAMMA = 0.9\n",
        "EPSILON = 0.999\n",
        "EPSILON_DECAY = 0.999\n",
        "EPSILON_MIN = 0.01\n",
        "EPISODES = 1000"
      ],
      "metadata": {
        "id": "KrvTXp1XEauz"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = Environment()\n",
        "agent = Agent(env, True)\n",
        "player = Agent(env, False)\n",
        "env.reset()"
      ],
      "metadata": {
        "id": "P7rCdI_Mprak"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_win = []\n",
        "\n",
        "for episode in range(EPISODES):\n",
        "    env.reset()\n",
        "    state = env.present_state\n",
        "    done = env.done\n",
        "\n",
        "    history = []\n",
        "\n",
        "    while not done:\n",
        "        agent_action = agent.get_action(state, env.available_actions)\n",
        "        next_state, reward, done, is_win = env.step(agent_action, True)\n",
        "        history.append((agent_action, reward))\n",
        "\n",
        "        if not done:\n",
        "            player_action = player.get_action(next_state, env.available_actions)\n",
        "            next_state, reward, done, is_win = env.step(player_action, False)\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "    agent.update_value_table(history)\n",
        "\n",
        "    total_win.append(is_win == \"win\")\n",
        "\n",
        "    if (episode+1) % 100 == 0:\n",
        "        print(f\"Episode: {episode+1}, win?: {is_win}, win_rate: {sum(total_win[episode-100:episode+1])/100}\")\n",
        "        env.render()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1bstnPRqq-z",
        "outputId": "6b2d0f57-eba7-4f51-d034-a0ab2d2920e8"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 100, win?: win, win_rate: 0.01\n",
            " X | . | .\n",
            "-----------\n",
            " . | X | .\n",
            "-----------\n",
            " O | O | X\n",
            "-----------\n",
            "\n",
            "Episode: 200, win?: win, win_rate: 0.84\n",
            " O | . | X\n",
            "-----------\n",
            " . | X | X\n",
            "-----------\n",
            " O | O | X\n",
            "-----------\n",
            "\n",
            "Episode: 300, win?: win, win_rate: 0.78\n",
            " X | O | X\n",
            "-----------\n",
            " . | X | O\n",
            "-----------\n",
            " X | . | O\n",
            "-----------\n",
            "\n",
            "Episode: 400, win?: win, win_rate: 0.85\n",
            " X | . | .\n",
            "-----------\n",
            " O | X | .\n",
            "-----------\n",
            " O | . | X\n",
            "-----------\n",
            "\n",
            "Episode: 500, win?: win, win_rate: 0.82\n",
            " X | . | .\n",
            "-----------\n",
            " O | X | .\n",
            "-----------\n",
            " O | . | X\n",
            "-----------\n",
            "\n",
            "Episode: 600, win?: win, win_rate: 0.85\n",
            " X | . | O\n",
            "-----------\n",
            " . | X | .\n",
            "-----------\n",
            " O | . | X\n",
            "-----------\n",
            "\n",
            "Episode: 700, win?: lose, win_rate: 0.8\n",
            " O | . | X\n",
            "-----------\n",
            " O | X | .\n",
            "-----------\n",
            " O | . | X\n",
            "-----------\n",
            "\n",
            "Episode: 800, win?: draw, win_rate: 0.84\n",
            " O | X | O\n",
            "-----------\n",
            " X | X | O\n",
            "-----------\n",
            " X | O | X\n",
            "-----------\n",
            "\n",
            "Episode: 900, win?: win, win_rate: 0.83\n",
            " X | . | .\n",
            "-----------\n",
            " . | X | .\n",
            "-----------\n",
            " O | O | X\n",
            "-----------\n",
            "\n",
            "Episode: 1000, win?: lose, win_rate: 0.85\n",
            " X | O | .\n",
            "-----------\n",
            " . | O | .\n",
            "-----------\n",
            " X | O | X\n",
            "-----------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lMLbZphlqm8d"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# minimax 알고리즘\n",
        "- 입력 받은 상태에서 얻을 수 있는 최대값이 뭔지 알려주는 함수\n",
        "- 개선점: 최대값을 얻을 수 있는 행동이 무엇인지 반환해야한다.\n",
        "- 현재 모든 경우의 수에 대해 계산하는 minimax 함수이다. 틱택토 정도의 작은 상황에서는 가능하지만, 상태의 수가 많아지면 depth를 도입해 일정 깊이만큼만 탐색하도록 해야한다."
      ],
      "metadata": {
        "id": "vgzsYaLOjm5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self, env, max_player:bool):\n",
        "        self.env = env\n",
        "        self.n = self.env.n\n",
        "        self.num_actions = self.env.num_actions\n",
        "        self.actions = torch.tensor(self.num_actions)\n",
        "\n",
        "        self.best_action = None\n",
        "\n",
        "\n",
        "    def minimax(self, present_state, depth, alpha, beta, max_player:bool):\n",
        "        temp_env = Environment()\n",
        "        state = copy.deepcopy(present_state)\n",
        "        temp_env.present_state = state\n",
        "        done, is_win = temp_env.is_done(state)\n",
        "        reward = 0\n",
        "\n",
        "        remain_actions = np.argwhere(state == 0)\n",
        "\n",
        "\n",
        "        if (done == True) or (depth == 0):\n",
        "            if is_win == \"win\":\n",
        "                reward = temp_env.reward_dict['win']\n",
        "\n",
        "            elif is_win == \"lose\":\n",
        "                reward = temp_env.reward_dict['lose']\n",
        "\n",
        "            else:\n",
        "                reward = temp_env.reward_dict['draw']\n",
        "\n",
        "            return reward\n",
        "\n",
        "\n",
        "        if max_player:\n",
        "            maxEval = -np.Inf\n",
        "            best_action = None\n",
        "            for (x, y) in remain_actions:\n",
        "                idx = self.n * x + y\n",
        "                child, _, _, _ = temp_env.step(idx, True)\n",
        "\n",
        "                eval = self.minimax(child, depth-1, alpha, beta, False)\n",
        "\n",
        "                if eval > maxEval:\n",
        "                    best_action = idx\n",
        "                    maxEval = eval\n",
        "\n",
        "                alpha = max(alpha, eval)\n",
        "                if beta <= alpha:\n",
        "                    break\n",
        "\n",
        "                if depth == DEPTH:  # 최상위 호출에서만 best_action을 저장\n",
        "                    self.best_action = best_action\n",
        "\n",
        "            return maxEval\n",
        "\n",
        "        else:\n",
        "            minEval = np.Inf\n",
        "            for (x, y) in remain_actions:\n",
        "                idx = self.n * x + y\n",
        "                child, _, _, _ = temp_env.step(idx, False)\n",
        "\n",
        "                eval = self.minimax(child, depth-1, alpha, beta, True)\n",
        "                minEval = min(minEval, eval)\n",
        "\n",
        "                beta = min(alpha, eval)\n",
        "                if beta <= alpha:\n",
        "                    break\n",
        "\n",
        "            return minEval\n",
        "\n",
        "\n",
        "    def get_action(self, state, agent_turn):\n",
        "        self.minimax(state, DEPTH, -np.Inf, np.Inf, True)\n",
        "        return self.best_action"
      ],
      "metadata": {
        "id": "rugve7NAjsAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEPTH = 100"
      ],
      "metadata": {
        "id": "bCdlr38KXMV0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}