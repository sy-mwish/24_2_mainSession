{"cells":[{"cell_type":"markdown","metadata":{"id":"VqbW4oJHV9xK"},"source":["# Import"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xV_VVBJ_V9Pz"},"outputs":[],"source":["import numpy as np\n","import random\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"]},{"cell_type":"markdown","metadata":{},"source":["# Import : by GitHub\n","\n","You can try this code in Colab by this method.  \n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19875,"status":"ok","timestamp":1715610586164,"user":{"displayName":"‎주민서(인공지능대학 인공지능학과)","userId":"08406903885902536930"},"user_tz":-540},"id":"FQroel6pV21d","outputId":"404116a3-cf07-4ddf-838a-cb9c301024d7"},"outputs":[],"source":["!git clone https://github.com/KanghwaSisters/24_2_mainSession.git "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","os.chdir('/content/24_2_mainSession/4주차/env') "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["! python GridWorldEnvironment.py # py file 실행"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from GridWorldEnvironment import GridWorldEnvironment"]},{"cell_type":"markdown","metadata":{"id":"xJi0rnmZWCgQ"},"source":["# load env"]},{"cell_type":"markdown","metadata":{"id":"zJ-tL9r4XG28"},"source":["- ⭐️ **파일을 사용하기 위해서는 cwd를 py가 있는 위치로 변경해야 한다.**\n","- 출처 : https://www.askpython.com/python/examples/import-py-files-google-colab"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JhosJDmfWuJN"},"outputs":[],"source":["import os\n","os.chdir('/content/drive/MyDrive/24-1/강화학습세션/환경')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_aWzRbCSWDwQ"},"outputs":[],"source":["! python GridWorldEnvironment3.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"63_3vBGkXz01"},"outputs":[],"source":["from GridWorldEnvironment3 import GridWorldEnvironment # ver2의 높이 너비 오류를 해결한 버전이 3"]},{"cell_type":"markdown","metadata":{"id":"7T8NNmZjn__7"},"source":["### DeepSARSA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aV-0hwc2oF8I"},"outputs":[],"source":["class DeepSARSA(nn.Module):\n","    def __init__(self, state_size, action_size):\n","        super().__init__()\n","        self.fc1 = nn.Linear(state_size, 30)\n","        self.fc2 = nn.Linear(30, 30)\n","        self.fc3 = nn.Linear(30, action_size)\n","\n","    def forward(self,x):\n","        x = self.fc1(x)\n","        x= self.fc2(x)\n","        x = self.fc3(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"jWMbPQmv8DcP"},"source":["### Agent_DS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WT9AFU728E99"},"outputs":[],"source":["class DeepSARSAAgent:\n","    def __init__(self, state_size, action_space):\n","        # 행동에 관한 파라미터\n","        self.action_space = action_space\n","        self.num_actions = len(action_space)\n","\n","        # 딥살사 하이퍼 파라미터\n","        self.step_size = 0.01\n","        self.discount_factor = 0.9\n","        self.epsilon = 1.0\n","        self.epsilon_decay = 0.999\n","        self.epsilon_min = 0.01\n","        self.learning_rate = 0.001\n","\n","        # 신경망\n","        self.model = DeepSARSA(state_size, self.num_actions)\n","        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=1e-3)\n","        self.loss = nn.MSELoss()\n","\n","    def get_action(self,state):\n","\n","        self.model.eval()\n","        state = torch.tensor(state, dtype=torch.float32)\n","\n","        if np.random.rand() <= self.epsilon:\n","            return random.randrange(self.num_actions)\n","        else:\n","            q_value = self.model(state)\n","            return torch.argmax(q_value).item()\n","\n","    def train_model(self, state, action_idx, reward, next_state, next_action_idx, done):\n","        if self.epsilon > self.epsilon_min:\n","            self.epsilon *= self.epsilon_decay\n","\n","        self.model.train()\n","\n","        state = torch.tensor(state, dtype=torch.float32)\n","        next_state = torch.tensor(next_state, dtype=torch.float32)\n","\n","        # 현재 state\n","        y_est = self.model(state)\n","        one_hot_idx = F.one_hot(torch.tensor([action_idx]), num_classes=self.num_actions)\n","        pred = torch.sum(y_est * one_hot_idx,axis=1)\n","\n","        # next state\n","        next_q = self.model(next_state)[next_action_idx]\n","        target = reward + (1-done) * self.discount_factor * next_q\n","\n","        cost = self.loss(pred, target)\n","\n","        self.optimizer.zero_grad()\n","        cost.backward()\n","        self.optimizer.step()"]},{"cell_type":"markdown","metadata":{"id":"M4UpGTEF8FWG"},"source":["### main_DS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3h3-vkH85n-u"},"outputs":[],"source":["# init value\n","env = GridWorldEnvironment(start_point=(0,0),\n","                           end_point=(4,4),\n","                           gridworld_size=(5,5))\n","\n","agent = DeepSARSAAgent(env.state_len, env.action_space)\n","\n","EPISODES = 1000\n","RENDER_PROCESS = False\n","RENDER_END = False\n","total_moves = []\n","check_point = {}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aTmITLRE5n-0","outputId":"fecd378f-8c74-4b69-dbfc-cee7fb67ced5"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"name":"stdout","output_type":"stream","text":["[Episode]: 100/1000 __ [Num of Moves mean]:32.7 __ [Epsilon]: 0.038\n","[Episode]: 200/1000 __ [Num of Moves mean]:8.7 __ [Epsilon]: 0.016\n","[Episode]: 300/1000 __ [Num of Moves mean]:8.1 __ [Epsilon]: 0.010\n","[Episode]: 400/1000 __ [Num of Moves mean]:8.2 __ [Epsilon]: 0.010\n"]}],"source":["# train code\n","\n","for episode in range(EPISODES):\n","    # 게임 환경을 초기화\n","    state = env.reset()\n","    moves_cnt = 0\n","    # 현재 상태에서 행동을 선택한다.\n","    action_idx = agent.get_action(state)\n","\n","    done = False\n","\n","    while not done:\n","        if RENDER_PROCESS:\n","            env.render() # 이동을 출력하기\n","\n","        # 취한 행동에 대한 next_state, reward, done을 환경이 제공한다.\n","        next_state, reward, done = env.step(action_idx)\n","\n","        # 다음 상태에서 행동을 선택한다.\n","        next_action_idx = agent.get_action(next_state)\n","\n","        # 큐함수를 업데이트한다.\n","        agent.train_model(state, action_idx, reward, next_state, next_action_idx, done)\n","\n","        state = next_state\n","        action_idx = next_action_idx\n","        moves_cnt += 1\n","\n","    total_moves.append(moves_cnt)\n","\n","    if (episode+1) % 100 == 0:\n","        print(f\"[Episode]: {episode+1}/{EPISODES} __ [Num of Moves mean]:{np.mean(total_moves[-100:]):.1f} __ [Epsilon]: {agent.epsilon:.3f}\")\n","        check_point[f'epi_{episode+1}'] = agent.model.state_dict()\n","\n","    if RENDER_END:\n","        env.render()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qPpkFK165n-0"},"outputs":[],"source":["# 마지막 학습 상황을 재현한다.\n","env.render()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bfaObC4o5n-0"},"outputs":[],"source":["# 1000 에피소드에서 에이전트의 이동 횟수를 시각화\n","plt.plot(total_moves)\n","plt.ylabel(\"cnt\")\n","plt.xlabel(\"episodes\")\n","plt.title(\"Num of Moves\")\n","plt.show()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
