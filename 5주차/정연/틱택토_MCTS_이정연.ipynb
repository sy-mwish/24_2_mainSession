{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1e6sO9vQ1OxrNYObGIHLYkV4-wmKl5ICi","timestamp":1727635408627},{"file_id":"13kZayQr9nmcdlmwWpbrVmqnJl4pgehPK","timestamp":1726932247642}],"collapsed_sections":["aozenopLpCvn"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 몬테 카를로 트리 탐색 tic-tac-toe agent"],"metadata":{"id":"BbotiJ3KbtWf"}},{"cell_type":"code","source":["#################\n","# 선공:  1, 'O' #\n","# 후공: -1, 'X' #\n","#################"],"metadata":{"id":"xbyMHzrLp5P0","executionInfo":{"status":"ok","timestamp":1728149287029,"user_tz":-540,"elapsed":271,"user":{"displayName":"‎이정연(인공지능대학 인공지능학과)","userId":"14307825210431520220"}}},"execution_count":72,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import math"],"metadata":{"id":"JXbCtaitQ_Ss","executionInfo":{"status":"ok","timestamp":1728149287339,"user_tz":-540,"elapsed":5,"user":{"displayName":"‎이정연(인공지능대학 인공지능학과)","userId":"14307825210431520220"}}},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":["## State"],"metadata":{"id":"FjF-DPiDo7yP"}},{"cell_type":"code","source":["BOARD_SIZE = (3,3)"],"metadata":{"id":"bmgUoBNBCDNi","executionInfo":{"status":"ok","timestamp":1728149287339,"user_tz":-540,"elapsed":4,"user":{"displayName":"‎이정연(인공지능대학 인공지능학과)","userId":"14307825210431520220"}}},"execution_count":74,"outputs":[]},{"cell_type":"code","source":["class State:\n","    def __init__(self, board_size=BOARD_SIZE, my_actions=None, enemy_actions=None):\n","        self.board_size = board_size # (3,3)\n","        self.num_actions = self.board_size[0] * self.board_size[1] # 3 * 3 = 9\n","        self.action_space = range(self.num_actions)\n","\n","        self.my_actions = [] if my_actions is None else my_actions\n","        self.enemy_actions = [] if enemy_actions is None else enemy_actions\n","\n","        self.board = self.create_board(self.my_actions, self.enemy_actions)\n","\n","        self.available_actions = self.get_available_actions()\n","\n","    def next(self, action):\n","        '''\n","        내 행동 이후 상대방 턴으로 변경\n","        '''\n","        my_actions = self.my_actions.copy()\n","        my_actions.append(action)\n","\n","        return State(self.board_size, self.enemy_actions, my_actions)\n","\n","    def create_board(self, my_actions, enemy_actions):\n","        total_board = np.zeros((2,self.board_size[0],self.board_size[1]))\n","\n","        my_board = np.zeros(self.board_size).flatten()\n","        enemy_board = np.zeros(self.board_size).flatten()\n","\n","        my_board[my_actions] = 1\n","        enemy_board[enemy_actions] = 1\n","\n","        total_board[0] = my_board.reshape(self.board_size)\n","        total_board[1] = enemy_board.reshape(self.board_size)\n","\n","        return total_board\n","\n","    def get_available_actions(self):\n","        my_actions_set = set(self.my_actions)\n","        enemy_actions_set = set(self.enemy_actions)\n","\n","        available_actions_set = set(range(self.num_actions)) - my_actions_set - enemy_actions_set\n","\n","        return list(available_actions_set)\n","\n","    def is_win(self):\n","        my_state = self.board[0]\n","\n","        row_win = np.sum(my_state, axis=0).max() == self.board_size[0]\n","        col_win = np.sum(my_state, axis=1).max() == self.board_size[1]\n","        diag_win = np.trace(my_state) == self.board_size[0]\n","        anti_diag_win = np.trace(np.fliplr(my_state)) == self.board_size[0]\n","\n","        return row_win or col_win or diag_win or anti_diag_win\n","\n","    def is_draw(self):\n","        return (np.sum(self.board[0]) + np.sum(self.board[1])) >= self.num_actions\n","\n","    def is_lose(self):\n","        enemy_state = self.board[1]\n","\n","        row_lose = np.sum(enemy_state, axis=0).max() == self.board_size[0]\n","        col_lose = np.sum(enemy_state, axis=1).max() == self.board_size[1]\n","        diag_lose = np.trace(enemy_state) == self.board_size[0]\n","        anti_diag_lose = np.trace(np.fliplr(enemy_state)) == self.board_size[0]\n","\n","        return row_lose or col_lose or diag_lose or anti_diag_lose\n","\n","    def is_done(self):\n","        return self.is_win() or self.is_draw() or self.is_lose()\n","\n","    def is_going_first(self):\n","        return len(self.my_actions) == len(self.enemy_actions)"],"metadata":{"id":"G_OHKemkBy7N","executionInfo":{"status":"ok","timestamp":1728149287339,"user_tz":-540,"elapsed":4,"user":{"displayName":"‎이정연(인공지능대학 인공지능학과)","userId":"14307825210431520220"}}},"execution_count":75,"outputs":[]},{"cell_type":"markdown","source":["## Environment"],"metadata":{"id":"gBvECaRBo9s1"}},{"cell_type":"code","source":["class TicTacToeEnv:\n","    def __init__(self):\n","        self.state = State()\n","\n","        self.reward = {'win': 10, 'lose': -10, 'draw': 0, 'continue': 0}\n","\n","    def reset(self):\n","        self.state = State()\n","        return self.state\n","\n","    def step(self, action):\n","        my_actions = self.state.my_actions.copy()\n","        enemy_actions = self.state.enemy_actions.copy()\n","\n","        my_actions.append(action)\n","\n","        next_state = State(self.board_size, my_actions, enemy_actions)\n","        self.state = State(self.board_size, self.state.enemy_actions, my_actions) # 다음 스텝 - 상대방 턴\n","\n","        if next_state.is_win():\n","            reward, done = self.reward['win'], True\n","\n","        elif next_state.is_draw():\n","            reward, done = self.reward['draw'], True\n","\n","        elif next_state.is_lose():\n","            reward, done = self.reward['lose'], True\n","\n","        else:\n","            reward, done = self.reward['continue'], False\n","\n","        return self.state, next_state, reward, done\n","\n","    def reset(self):\n","        self.state = State()\n","        return self.state\n","\n","    def render(self, state):\n","        board = state.board[0] + (-1 * state.board[1]) if state.is_going_first() else state.board[1] + (-1 * state.board[0])\n","\n","        int_to_symbol = np.where(board == 1, 'O',\n","                                np.where(board == -1, 'X', '-'))\n","\n","        rendering_board = '\\n'.join([' '.join(row) for row in int_to_symbol])\n","\n","        print()\n","        print(rendering_board)\n","        print()"],"metadata":{"id":"MkZ47mFvAAeX","executionInfo":{"status":"ok","timestamp":1728149287339,"user_tz":-540,"elapsed":4,"user":{"displayName":"‎이정연(인공지능대학 인공지능학과)","userId":"14307825210431520220"}}},"execution_count":76,"outputs":[]},{"cell_type":"markdown","source":["## MCTS Agent"],"metadata":{"id":"oDhBf-A_pAQs"}},{"cell_type":"code","source":["NUM_OF_SIMULATION = 100"],"metadata":{"id":"vO4SUAodYbF7","executionInfo":{"status":"ok","timestamp":1728149287339,"user_tz":-540,"elapsed":4,"user":{"displayName":"‎이정연(인공지능대학 인공지능학과)","userId":"14307825210431520220"}}},"execution_count":77,"outputs":[]},{"cell_type":"code","source":["class MCTSAgent:\n","    def __init__(self, env):\n","        self.env = env\n","        self.num_of_simulation = NUM_OF_SIMULATION\n","\n","    def mcts_action(self, state):\n","        # 몬테카를로 트리 탐색의 노드 정의\n","        class Node:\n","            # 노드 초기화\n","            def __init__(self, state, agent):\n","                self.state = state # 상태\n","                self.agent = agent # MCTSAgent 인스턴스\n","                self.w = 0 # 보상 누계\n","                self.n = 0 # 시행 횟수\n","                self.child_nodes = None  # 자녀 노드 군\n","\n","            # 국면 가치 계산\n","            def evaluate(self):\n","                # 게임 종료 시\n","                if self.state.is_done():\n","                    # 승패 결과로 가치 취득\n","                    value = -1 if self.state.is_lose() else 0 # 패배 시 -1, 무승부 시 0\n","\n","                    # 보상 누계와 시행 횟수 갱신\n","                    self.w += value\n","                    self.n += 1\n","                    return value\n","\n","                # 자녀 노드가 존재하지 않는 경우\n","                if not self.child_nodes:\n","                    # 플레이아웃으로 가치 얻기\n","                    value = self.agent.playout(self.state)  # self.playout -> self.agent.playout로 수정\n","\n","                    # 보상 누계와 시행 횟수 갱신\n","                    self.w += value\n","                    self.n += 1\n","\n","                    # 자녀 노드 전개\n","                    if self.n == 10:\n","                        self.expand()\n","                    return value\n","\n","                # 자녀 노드가 존재하는 경우\n","                else:\n","                    # UCB1이 가장 큰 자녀 노드를 평가해 가치 얻기\n","                    value = -self.next_child_node().evaluate()\n","\n","                    # 보상 누계와 시행 횟수 갱신\n","                    self.w += value\n","                    self.n += 1\n","                    return value\n","\n","            # 자녀 노드 전개\n","            def expand(self):\n","                available_actions = self.state.available_actions\n","                self.child_nodes = []\n","                for action in available_actions:\n","                    self.child_nodes.append(Node(self.state.next(action), self.agent))  # 자녀 노드 생성 시에도 agent 전달\n","\n","            # UCB1이 가장 큰 자녀 노드 얻기\n","            def next_child_node(self):\n","                # 시행 횟수가 0인 자녀 노드 반환\n","                for child_node in self.child_nodes:\n","                    if child_node.n == 0:\n","                        return child_node\n","\n","                # UCB1 계산\n","                t = 0\n","                for c in self.child_nodes:\n","                    t += c.n\n","                ucb1_values = []\n","                for child_node in self.child_nodes:\n","                    ucb1_values.append(-child_node.w/child_node.n+(2*math.log(t)/child_node.n)**0.5)\n","\n","                # UCB1이 가장 큰 자녀 노드 반환\n","                return self.child_nodes[self.agent.argmax(ucb1_values)]    # self.argmax -> self.agent.argmax로 수정\n","\n","        # 현재 국면의 노드 생성\n","        root_node = Node(state, self)  # Node 생성 시 agent(self) 전달\n","        root_node.expand()\n","\n","        # 100회 시뮬레이션 실행\n","        for _ in range(self.num_of_simulation):\n","            root_node.evaluate()\n","\n","        # 시행 횟수가 가장 큰 값을 갖는 행동 반환\n","        available_actions = state.available_actions\n","        n_list = []\n","        for c in root_node.child_nodes:\n","            n_list.append(c.n)\n","\n","        print('------------------')\n","        print(f\"Available_actions : {available_actions}\")\n","        print(f\"Value_per_action : {n_list}\")\n","\n","        return available_actions[self.argmax(n_list)]\n","\n","    def playout(self, state):\n","        if state.is_lose():\n","            return self.env.reward['lose']\n","\n","        if state.is_draw():\n","            return  self.env.reward['draw']\n","\n","        # 다음 상태의 상태 평가\n","        return -self.playout(state.next(self.random_action(state)))\n","\n","    def random_action(self, state):\n","        available_actions = state.available_actions\n","        return np.random.choice(available_actions)\n","\n","    def argmax(self, collection):\n","        max_idx_list = np.arange(len(collection))[collection == np.max(collection)]\n","        return np.random.choice(max_idx_list)"],"metadata":{"id":"g-4u3THItzSn","executionInfo":{"status":"ok","timestamp":1728149287717,"user_tz":-540,"elapsed":381,"user":{"displayName":"‎이정연(인공지능대학 인공지능학과)","userId":"14307825210431520220"}}},"execution_count":78,"outputs":[]},{"cell_type":"markdown","source":["## Other Agents"],"metadata":{"id":"aozenopLpCvn"}},{"cell_type":"code","source":["class RandomAgent:\n","    def __init__(self, env):\n","        self.env = env\n","\n","    def random_action(self, state):\n","        available_actions = state.available_actions\n","        return np.random.choice(available_actions)"],"metadata":{"id":"u-sKGkkpojO0","executionInfo":{"status":"ok","timestamp":1728149287717,"user_tz":-540,"elapsed":6,"user":{"displayName":"‎이정연(인공지능대학 인공지능학과)","userId":"14307825210431520220"}}},"execution_count":79,"outputs":[]},{"cell_type":"code","source":["class AlphaBetaAgent:\n","    def __init__(self, env):\n","        self.env = env\n","\n","    def random_action(self, state):\n","        available_actions = state.available_actions\n","        return np.random.choice(available_actions)\n","\n","    def alpha_beta(self, state, alpha, beta):\n","        if state.is_lose():\n","            return -1\n","\n","        if state.is_draw():\n","            return 0\n","\n","        score = 0\n","        for action in state.available_actions:\n","            score = -self.alpha_beta(state.next(action), -beta, -alpha)\n","            if score > alpha:\n","                alpha = score\n","\n","            if alpha >= beta:\n","                return alpha\n","\n","        return alpha\n","\n","    def alpha_beta_action(self, state):\n","        best_action = 0\n","        alpha = -np.inf\n","        for action in state.legal_actions():\n","            score = -self.alpha_beta(state.next(action), -np.inf, -alpha)\n","            if score > alpha:\n","                best_action = action\n","                alpha = score\n","\n","        return best_action"],"metadata":{"id":"xJPvAjlqopuH","executionInfo":{"status":"ok","timestamp":1728149287718,"user_tz":-540,"elapsed":7,"user":{"displayName":"‎이정연(인공지능대학 인공지능학과)","userId":"14307825210431520220"}}},"execution_count":80,"outputs":[]},{"cell_type":"code","source":["NUM_OF_PLAYOUT = 10"],"metadata":{"id":"u6kvPyO3sQpt","executionInfo":{"status":"ok","timestamp":1728149287718,"user_tz":-540,"elapsed":7,"user":{"displayName":"‎이정연(인공지능대학 인공지능학과)","userId":"14307825210431520220"}}},"execution_count":81,"outputs":[]},{"cell_type":"code","source":["class MCSAgent:\n","    def __init__(self, env):\n","        self.env = env\n","        self.num_of_playout = NUM_OF_PLAYOUT\n","\n","    def playout(self, state):\n","        if state.is_lose():\n","            return self.env.reward['lose']\n","\n","        if state.is_draw():\n","            return  self.env.reward['draw']\n","\n","        # 다음 상태의 상태 평가\n","        return -self.playout(state.next(self.random_action(state)))\n","\n","    def random_action(self, state):\n","        available_actions = state.available_actions\n","        return np.random.choice(available_actions)\n","\n","    def mcs_action(self, state):\n","        available_actions = state.available_actions\n","        values = np.zeros(shape=len(available_actions))\n","\n","        for i, action in enumerate(available_actions):\n","            for _ in range(self.num_of_playout):\n","                values[i] += - self.playout(state.next(action))\n","            values[i] /= self.num_of_playout\n","\n","        print('------------------')\n","        print(f\"Available_actions : {available_actions}\")\n","        print(f\"Value_per_action : {values}\")\n","        return available_actions[self.argmax(values)]\n","\n","    def argmax(self, collection):\n","        max_idx_list = np.arange(len(collection))[collection == np.max(collection)]\n","        return np.random.choice(max_idx_list)"],"metadata":{"id":"xXPI9lyCotsv","executionInfo":{"status":"ok","timestamp":1728149287718,"user_tz":-540,"elapsed":7,"user":{"displayName":"‎이정연(인공지능대학 인공지능학과)","userId":"14307825210431520220"}}},"execution_count":82,"outputs":[]},{"cell_type":"markdown","source":["## Main"],"metadata":{"id":"Xeu86H1ypHlx"}},{"cell_type":"code","source":["env = TicTacToeEnv()\n","state = State()\n","agent_1 = MCTSAgent(env=env)\n","agent_2 = RandomAgent(env=env)\n","\n","while True:\n","    if state.is_done():\n","        break\n","\n","    if state.is_going_first():\n","        action = agent_1.mcts_action(state)\n","\n","    else:\n","        action = agent_2.random_action(state)\n","\n","    state = state.next(action)\n","\n","    print()\n","    print(f\"Action : {action}\")\n","    env.render(state)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eRRVWhoBdqEx","executionInfo":{"status":"ok","timestamp":1728149287718,"user_tz":-540,"elapsed":6,"user":{"displayName":"‎이정연(인공지능대학 인공지능학과)","userId":"14307825210431520220"}},"outputId":"ecdbf1e6-5aac-4a21-afb5-efde66321880"},"execution_count":83,"outputs":[{"output_type":"stream","name":"stdout","text":["------------------\n","Available_actions : [0, 1, 2, 3, 4, 5, 6, 7, 8]\n","Value_per_action : [4, 1, 14, 1, 54, 4, 1, 1, 20]\n","\n","Action : 4\n","\n","- - -\n","- O -\n","- - -\n","\n","\n","Action : 1\n","\n","- X -\n","- O -\n","- - -\n","\n","------------------\n","Available_actions : [0, 2, 3, 5, 6, 7, 8]\n","Value_per_action : [30, 42, 2, 2, 2, 1, 21]\n","\n","Action : 2\n","\n","- X O\n","- O -\n","- - -\n","\n","\n","Action : 5\n","\n","- X O\n","- O X\n","- - -\n","\n","------------------\n","Available_actions : [0, 3, 6, 7, 8]\n","Value_per_action : [49, 1, 1, 1, 48]\n","\n","Action : 0\n","\n","O X O\n","- O X\n","- - -\n","\n","\n","Action : 7\n","\n","O X O\n","- O X\n","- X -\n","\n","------------------\n","Available_actions : [8, 3, 6]\n","Value_per_action : [1, 98, 1]\n","\n","Action : 3\n","\n","O X O\n","O O X\n","- X -\n","\n","\n","Action : 8\n","\n","O X O\n","O O X\n","- X X\n","\n","------------------\n","Available_actions : [6]\n","Value_per_action : [100]\n","\n","Action : 6\n","\n","O X O\n","O O X\n","O X X\n","\n"]}]}]}