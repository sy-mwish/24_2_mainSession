{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q62rPCNVFQf0",
        "outputId": "33318c55-89db-4c48-d736-929863b9ca51"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/KanghwaSisters/24_2_mainSession.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8jzJChFEaUN",
        "outputId": "167ed6dc-e60e-43a5-b89e-c22d4bdcb845"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path '24_2_mainSession' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/24_2_mainSession/4주차/env')"
      ],
      "metadata": {
        "id": "ajH6tZSYEaJ6"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! python GridWorldEnvironment.py"
      ],
      "metadata": {
        "id": "PD4V70oMEaDi"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from GridWorldEnvironment import GridWorldEnvironment"
      ],
      "metadata": {
        "id": "i2jagNVmEZ7j"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = GridWorldEnvironment(start_point=(0,0),\n",
        "                           end_point=(4,4),\n",
        "                           gridworld_size=(5,5))"
      ],
      "metadata": {
        "id": "5SikGnBhEZnz"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep SARSA Class"
      ],
      "metadata": {
        "id": "C3iT8H-CG39s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "hmi0BwlHCwFz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "import pylab\n",
        "\n",
        "# 딥살사 인공신경망(입력값: 상태 / 출력: 각 행동에 대한 큐함수)\n",
        "class DeepSARSA(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(DeepSARSA, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 30)  # 입력층\n",
        "        self.fc2 = nn.Linear(30, 30)  # 은닉층\n",
        "        self.fc_out = nn.Linear(30, action_size)  # 출력층\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        q = self.fc_out(x)\n",
        "        return q\n",
        "\n",
        "# 그리드월드 예제에서의 딥살사 에이전트\n",
        "class DeepSARSAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        # 상태의 크기와 행동의 크기 정의\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # 딥살사 하이퍼 파라미터\n",
        "        self.discount_factor = 0.99 #할인율\n",
        "        self.learning_rate = 0.001\n",
        "        self.epsilon = 1.  #e-탐욕 정책: 처음에는 무조건 탐험한다\n",
        "        self.epsilon_decay = .9999 #엡실론 값을 점차 낮춘다\n",
        "        self.epsilon_min = 0.01\n",
        "\n",
        "        #인공신경망 생성\n",
        "        self.model = DeepSARSA(state_size, action_size)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), learning_rate=self.learning_rate)\n",
        "\n",
        "    # e-탐욕 정책에 따라서 행동을 반환\n",
        "    def get_action(self, state):\n",
        "        if np.random.rand() <= self.epsilon: #0~1 사이 무작위 값과 엡실론 값 비교\n",
        "            return random.randrange(self.action_size) #엡실론 값이 더 크다면 무작위(임의의) 행동 선택\n",
        "        else: #무작위 값이 더 크다면 탐욕적인 행동 선택\n",
        "            state = torch.FloatTensor(state)\n",
        "            q_values = self.model(state) #현재 state에 대한 큐함수 값 출력\n",
        "            return torch.argmax(q_values).item() #이 큐함수 값 중 가장 큰 값을 가지는 행동 반환\n",
        "\n",
        "    # <s, a, r, s', a'>의 샘플로부터 인공신경망 업데이트\n",
        "    def train_model(self, state, action, reward, next_state, next_action, done):\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "        #학습 파라미터\n",
        "        model_params=self.model.trainable_variables\n",
        "\n",
        "        # 학습 파라미터- 상태와 다음 상태를 텐서로 변환\n",
        "        next_state = torch.FloatTensor(next_state)\n",
        "\n",
        "        predict = q_values[0] #예측/ 현재 상태에서의 예측 큐함수 값\n",
        "        one_hot_action = F.one_hot(torch.tensor(action), self.action_size) # 실제로 한 행동에 해당하는 값만 추출하기 위해\n",
        "        predict = torch.sum(one_hot_action*predict, axis=1) # 실제 행동에 대한 모델의 출력만\n",
        "\n",
        "        # done = True 일 경우 에피소드가 끝나서 다음 상태가 없음\n",
        "        with torch.no_grad():\n",
        "          next_q_values = self.model(next_state)\n",
        "          next_q = next_q_values[0][next_action] #다음 상태에서 다음 행동을 취했을 때 큐함수 값\n",
        "        target = reward + (1 - done) * self.discount_factor * next_q #정답/ done=True일 경우 target=reward\n",
        "\n",
        "\n",
        "        # MSE 오류함수 계산\n",
        "        loss = nn.MSE(predict, torch.FloatTensor([target]))\n",
        "\n",
        "        # 역전파 및 가중치 업데이트\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "lYT9r3p6HK3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Deep SARSA**를 이용해 그리드 월드 학습시키기  \n",
        "- 학습 지표 시각화"
      ],
      "metadata": {
        "id": "YfLK-zbsHP6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\": #환경과 에이전트가 상호작용\n",
        "    # 환경 생성\n",
        "    env = GridWorldEnvironment(start_point=(0,0),\n",
        "                           end_point=(4,4),\n",
        "                           gridworld_size=(5,5))\n",
        "    state_size= env.state_len # 상태 공간 크기\n",
        "    action_size= env.action_space # 행동 공간의 크기를 정수로 변환\n",
        "\n",
        "    #에이전트 생성\n",
        "    agent = DeepSARSAgent(state_size, action_size)\n",
        "\n",
        "    scores = [] # 각 에피소드의 총 보상을 기록할 리스트\n",
        "\n",
        "    for e in range(1000):\n",
        "        done = False\n",
        "        state = env.reset() # 환경 초기화\n",
        "        state = np.identity(state_size)[state:state + 1]  # 상태를 one-hot 인코딩으로 변환\n",
        "        score = 0 # 에피소드의 총 보상을 초기화\n",
        "\n",
        "        while not done:\n",
        "            # 현재 상태에 대한 행동 선택\n",
        "            action = agent.get_action(state)\n",
        "\n",
        "            # 선택한 행동으로 환경에서 한 타임스텝 진행 후 샘플 수집\n",
        "            next_state, reward, done = env.step(action)\n",
        "            next_state = np.identity(state_size)[next_state:next_state + 1]  # 다음 상태 변환\n",
        "            next_action = agent.get_action(next_state) #다음 상태에 대해서 다음 행동을 구함\n",
        "\n",
        "            # 샘플로 모델 학습\n",
        "            agent.train_model(state, action, reward, next_state,\n",
        "                                next_action, done) #살사니까 다음 상태의 다음 행동까지 다 고려해서 모델 학습\n",
        "            score += reward #보상 누적\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                # 에피소드마다 학습 결과 출력\n",
        "                print(\"episode: {:3d} | score: {:3d} | epsilon: {:.3f}\".format(\n",
        "                      e + 1, score, agent.epsilon))\n",
        "                scores.append(score) #에피소드 당 총 보상 저장\n",
        "                episodes.append(e + 1)\n",
        "\n",
        "                #학습 성과를 시각화\n",
        "                pylab.plot(episodes, scores, 'b')\n",
        "                pylab.xlabel(\"episode\")\n",
        "                pylab.ylabel(\"score\")\n",
        "                plt.title('Score per Episode')\n",
        "                plt.show()\n",
        "\n",
        "\n",
        "        # 100 에피소드마다 모델 저장\n",
        "        if e % 100 == 0:\n",
        "            torch.save(agent.model.state_dict(), 'save_model/model_{}.pth'.format(e))\n",
        "            print(\"Model saved at episode: {}\".format(e))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "lxKXDx4EHE07",
        "outputId": "bf3260e6-4182-4e90-e6ba-0f7103b74549"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, torch.memory_format memory_format = None, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-6e558cd73fea>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#에이전트 생성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeepSARSAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# 각 에피소드의 총 보상을 기록할 리스트\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-82-5fdd51bb4832>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, state_size, action_size)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m#인공신경망 생성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeepSARSA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-82-5fdd51bb4832>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, state_size, action_size)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 입력층\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 은닉층\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 출력층\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfactory_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfactory_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, torch.memory_format memory_format = None, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n"
          ]
        }
      ]
    }
  ]
}